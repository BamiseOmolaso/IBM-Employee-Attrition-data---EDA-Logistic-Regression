# -*- coding: utf-8 -*-
"""Simplilearn Project on IBM Employee Data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18vro-r_nSuIwEW2U05dtUYpB41cuCXxi
"""

# IBM HR Analytics Employee Attrition Modeling

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

#Loading my dataset

df = pd.read_csv('/content/IBM Attrition Data.csv')

df.head()

type(df)

# Shape of the dataset

df.shape

df.info

#Attribute type 

df.dtypes

df.columns

#Checking for missing values

df.isnull().sum()

#Checking for missing values

df.isnull().values.any()

df['Age'].unique()

#Distribution of numerical variables

df.describe()

# Distribution of Age of Employees

plt.figure(figsize=(10,8))
df['Age'].hist(bins=80)
plt.title("Age distribution of Employees")
plt.xlabel("Age")
plt.ylabel("Number of Employees")
plt.show()

# Explore data for Attrition by Age

plt.figure(figsize=(12,10))
plt.scatter(df.Attrition,df.Age, alpha=.5)
plt.title("Attrition by Age ")
plt.ylabel("Age")
plt.show()

# explore data for Left employees breakdown
plt.figure(figsize=(10,8))
df.Attrition.value_counts().plot(kind='bar',color='r')
plt.title("Attrition Analysis ")
plt.show()

# explore data for Education Field distribution

plt.figure(figsize=(12,8))
df.EducationField.value_counts().plot(kind='barh',color='m',alpha=.5)
plt.title("Education Field Distribution")
plt.show()

# explore data for Marital Status
plt.figure(figsize=(10,8))
df.MaritalStatus.value_counts().plot(kind='barh', color='g', alpha=.5)
plt.show()

df.corr()

# Plotting a correlation heatmap

plt.figure(figsize=(30,15))
sns.heatmap(df.corr(), vmax=1, square=True, annot=True, cmap='viridis')
plt.title('Correlation between attributes')
plt.show()

"""## Build up a logistic regression model to predict which employees are likely to attrite"""

#Import modules

import matplotlib
from pandas.plotting import scatter_matrix

import seaborn as sns
sns.set(style='white', color_codes=True)
sns.set(font_scale=1.5)

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report, precision_score, accuracy_score, recall_score, f1_score
from sklearn import metrics

df.Attrition.value_counts()

x1 = df.drop(['Attrition'], axis=1)
x1

#Selecting features for training
x = pd.get_dummies(x1)
x

#Select the target variable (Dependent variable)

y = df['Attrition']
y

#Convert the categorical input into numerical

y.replace('Yes',1, inplace=True)
y.replace('No',0, inplace=True)

print(x.shape)
print(y.shape)

#Split the data into training and test data

x_train,x_test,y_train,y_test = train_test_split(x,y, test_size=0.3, random_state=100)

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

#Instantiate the Classifier model

model = LogisticRegression()

#Fit the model on the training data

model.fit(x_train, y_train)

y_pred = model.predict(x_test)

cm = metrics.confusion_matrix(y_test, y_pred)

cm

cm = metrics.confusion_matrix(y_test, y_pred, labels=[0,1])

df_cm = pd.DataFrame(cm, index = [i for i in ['0', '1']],
                     columns = [i for i in ['Predict 0', 'Predict 1']])
plt.figure(figsize = (7,5))
sns.heatmap(df_cm, annot=True)

metrics.accuracy_score(y_test, y_pred)

print(classification_report(y_test, y_pred))

model.coef_

model.intercept_

model.predict_proba(x_test)